{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the PySPI tutorial!\n",
    "\n",
    "This tutorial will show you how to get started with `pyspi` and to use it for classifying your own multivariate time series (MTS).\n",
    "\n",
    "## Computing all pairwise interactions for one MTS\n",
    "\n",
    "The first part of the tutorial illustrates how to use the `Calculator` class to compute all SPIs.\n",
    "\n",
    "Before we get started, we will need to download the data: the daily open price of five popular stocks (known as the *FAANG* index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" First, we need to set up some tools for downloading and minimally processing the data \n",
    "\"\"\"\n",
    "import datetime, warnings\n",
    "import pandas_datareader as pdr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from scipy.signal import detrend\n",
    "\n",
    "def download(symbols,start,days):\n",
    "    \"\"\" Download financial data from one of two sources, Yahoo Finance (yahoo) and the St. Lois Federal Reserve (fred)\n",
    "    \"\"\"\n",
    "    end = start + datetime.timedelta(days=days)\n",
    "\n",
    "    startstr = start.strftime('%Y-%m-%d')\n",
    "    endstr = end.strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f'Obtaining {symbols} data from {startstr} to {endstr}...')\n",
    "    try:\n",
    "        # Attempting to use Yahoo Finance...\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.simplefilter('always')\n",
    "            close = pdr.DataReader(symbols, 'yahoo', startstr, endstr)['Close']\n",
    "    except:\n",
    "        # Yahoo didn''t work. Switching to FRED.\n",
    "        close = pdr.DataReader(symbols, 'fred', startstr, endstr)\n",
    "\n",
    "    # Match data up with weekdays\n",
    "    weekdays = pd.date_range(start=startstr, end=endstr, freq='B')\n",
    "    close = close.reindex(weekdays)\n",
    "\n",
    "    # For any NaN's, propogate last valid value forward (and remove first value) \n",
    "    z = close.fillna(method='ffill').values.T[:,2:]\n",
    "\n",
    "    # Make sure to always detrend and normalise your data, otherwise most statistics will give spurious results.\n",
    "    return detrend(zscore(z,ddof=1,nan_policy='omit',axis=1))\n",
    "\n",
    "# The FAANG tickers (Facebook/Meta, Amazon, Apple, Netflix, Google)\n",
    "stocks = ['FB','AMZN','AAPL','NFLX','GOOGL'] \n",
    "\n",
    "# We'll download 140 days of data (corresponding to ~100 observations from business days)\n",
    "ndays = 140 \n",
    "\n",
    "# Set a recent(ish) starting date for the period\n",
    "start_datetime = datetime.datetime.strptime('2014-01-01', '%Y-%m-%d') # Earliest date we will sample\n",
    "\n",
    "print('Begin data download.')\n",
    "z = download(stocks,start_datetime,ndays)\n",
    "print(f'Done. Obtained MTS of size {z.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Now we've got our data, we can inspect it to make sure everything looks OK.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_data(z,labels):\n",
    "    plt.subplots()\n",
    "    plt.pcolormesh(z,vmin=-1.5,vmax=1.5,cmap=sns.color_palette('icefire_r',as_cmap=True))\n",
    "    plt.colorbar()\n",
    "\n",
    "    ticks = [t+0.5 for t in range(len(labels))]\n",
    "    plt.yticks(ticks=ticks, labels=labels)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Symbol')\n",
    "    plt.show()\n",
    "\n",
    "plot_data(z,stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Now that we have our data, and inspected it to make sure it looks OK, we can compute all pairwise interactions.\n",
    "\"\"\"\n",
    "\n",
    "from pyspi.calculator import Calculator\n",
    "\n",
    "# These two lines show the main usage of the calculator: simply instantiate and compute.\n",
    "calc = Calculator(z)\n",
    "calc.compute()\n",
    "\n",
    "# We can now inspect the results table, which includes hundreds of pairwise interactions\n",
    "print(calc.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" One purpose of the calculator is that we can now extract pairwise matrices for every type of interaction.\n",
    "\n",
    "For instance, below we will examine how covariance, dynamic time warping, and Granger causality differs when computing the relationship between the FAANG stock-market data.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "def plot_mpi(S,identifier,labels,ax=None):\n",
    "    \"\"\" Plot a given matrix of pairwise interactions, annotating the process labels and identifier\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    plt.sca(ax)\n",
    "\n",
    "    # Use a diverging cmap if our statistic goes negative (and a sequential cmap otherwise)\n",
    "    if np.nanmin(S) < 0.:\n",
    "        maxabsval = max(abs(np.nanmin(S)),abs(np.nanmax(S)))\n",
    "        norm = colors.Normalize(vmin=-maxabsval, vmax=maxabsval)\n",
    "        plt.imshow(S,cmap='coolwarm',norm=norm)\n",
    "    else:\n",
    "        plt.imshow(S,cmap='Reds',vmin=0)\n",
    "\n",
    "    plt.xticks(ticks=range(len(stocks)),labels=labels,rotation=45)\n",
    "    plt.yticks(ticks=range(len(stocks)),labels=labels)\n",
    "    plt.xlabel('Symbol')\n",
    "    plt.ylabel('Symbol')\n",
    "    plt.title(identifier)\n",
    "    plt.colorbar()\n",
    "\n",
    "# Iterate through the three methods (covariance, dynamic time warping, Granger causality, and convergent cross-mapping), extract and plot their matrices\n",
    "spis = ['cov_EmpiricalCovariance','dtw_constraint-itakura','gc_gaussian_k-max-10_tau-max-2','ccm_E-None_max']\n",
    "for identifier in spis:\n",
    "    # Simply index an SPI in the output table, which will give you an MxM dataframe of pairwise interactions (where M is the number of processes)\n",
    "    S = calc.table[identifier]\n",
    "\n",
    "    # Plot this dataframe\n",
    "    plot_mpi(S,identifier,stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying MTS\n",
    "\n",
    "Now that you know how to compute the hundreds of pairwise interactions from data, let's put them to the test!\n",
    "\n",
    "This part of the tutorial illustrates how to use `sklearn` to classify between two types of time series using a comprehensive representation of their pairwise interactions.\n",
    "\n",
    "Here, we will try to delineate the stock-market data from earlier from foreign exchange-rate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" First, we need to download several instances of the data type for training/testing our classifier.\n",
    "\n",
    "This may take a while..\n",
    "\"\"\"\n",
    "\n",
    "# Add in some more symbols for the market rates\n",
    "forex = ['DEXJPUS','DEXUSEU','DEXCHUS','DEXUSUK','DEXCAUS']\n",
    "\n",
    "ninstances = 20\n",
    "\n",
    "# Dict to store our results (given by the key (type,starting date))\n",
    "database = {}\n",
    "for i in range(ninstances):\n",
    "    # Iterate the start datetime by ndays for each instance\n",
    "    start_datetime = start_datetime + datetime.timedelta(days=ndays)\n",
    "    print(f'[{i}/{ninstances-1}] Downloading data for the 140 period starting at {start_datetime}.')\n",
    "\n",
    "    # Download the stock data (and plot)\n",
    "    database[('stocks',start_datetime)] = download(stocks,start_datetime,ndays)\n",
    "    plot_data(database[('stocks',start_datetime)],stocks)\n",
    "    \n",
    "    # Download the forex data (and plot)\n",
    "    database[('forex',start_datetime)] = download(forex,start_datetime,ndays)\n",
    "    plot_data(database[('forex',start_datetime)],forex)\n",
    "\n",
    "print(f'Successfully downloaded {ninstances} datasets of both types of data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Now, compute all pairwise interactions from all datasets (storing the results)\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "\n",
    "# First, let's create one calculator and copy it for each MTS so that we don't need to re-initialise each time\n",
    "calc = Calculator()\n",
    "\n",
    "results = {}\n",
    "for key in database:\n",
    "\n",
    "    # Copy the top-level calculator\n",
    "    mycalc = deepcopy(calc)\n",
    "\n",
    "    # Name the calculator after the type of data (stock/forex) plus the starting date\n",
    "    mycalc.name = key[0] + '_' + key[1].strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Ensure we have no NaNs in our dataset (preferably using a more clever way of data imputation than this one)\n",
    "    dataset = np.nan_to_num(database[key])\n",
    "\n",
    "    # Load the dataset\n",
    "    mycalc.load_dataset(dataset)\n",
    "\n",
    "    # Compute all pairwise interactions\n",
    "    mycalc.compute()\n",
    "\n",
    "    # Store our results\n",
    "    results[key] = mycalc.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Can we tell if it's ForEx data or stock-market data based on a covariance matrix?\n",
    "\"\"\"\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def get_data_matrix(table,spi):\n",
    "    \"\"\" For a given SPI, extract the off-diagonals as feature vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # For each dataset in the results table...\n",
    "    for k in table:\n",
    "        # ...Extract the matrix corresponding to this SPI...\n",
    "        A = table[k][spi].values\n",
    "\n",
    "        # ...Vectorise and join the upper and lower triangles to form a feature vector...\n",
    "        x = np.concatenate([A[np.triu_indices(A.shape[0],1)],\n",
    "                            A[np.tril_indices(A.shape[0],-1)]])\n",
    "        x = np.atleast_2d(x)\n",
    "\n",
    "        # ...and concatenate each feature vector together to form a data matrix\n",
    "        try:\n",
    "            X = np.concatenate([X,x], axis=0)\n",
    "        except NameError:\n",
    "            X = x\n",
    "    return X\n",
    "\n",
    "def get_accuracy(clf,X,y):\n",
    "    nrepeats = 5\n",
    "    score = np.full(nrepeats,np.nan)\n",
    "    for r in range(nrepeats):\n",
    "        # Convert the data into half-half train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=r)\n",
    "\n",
    "        # Normalise the features so that the SVM behaves well\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "        # Fit the classifier\n",
    "        clf.fit(X_train,y_train)\n",
    "\n",
    "        # Scale the test data (with the same scaler as above)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Predict the output\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Get the accuracy for this train/test split (should be balanced)\n",
    "        score[r] = balanced_accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Return the average score\n",
    "    return np.nanmean(score)\n",
    "\n",
    "# First check with the standard covariance matrix\n",
    "spi = 'cov_EmpiricalCovariance'\n",
    "X = get_data_matrix(results,spi)\n",
    "\n",
    "# Set up the dataset labels for classification (\"forex\" == 0, \"stocks\" == 1)\n",
    "y = np.array([int(key[0] == 'stocks') for key in results])\n",
    "\n",
    "# Linear SVM classifier using the default settings\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "score = get_accuracy(clf,X,y)\n",
    "\n",
    "print(f'Average accuracy for {spi} in delineating ForEx from FAANG data: {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Let's try this out for every SPI and see how they all perform\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Repeat last cell for each SPI\n",
    "scores = {}\n",
    "spis = mycalc.spis.keys()\n",
    "for spi in spis:\n",
    "    X = get_data_matrix(results,spi)\n",
    "    try:\n",
    "        scores[spi] = get_accuracy(clf, X, y)\n",
    "        print(f'Average score for {spi}: {scores[spi]}')\n",
    "    except ValueError as err:\n",
    "        print(f'Issue with SPI {spi}: {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" And plot the results\n",
    "\"\"\"\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Get a histogram \n",
    "sns.histplot(scores)\n",
    "plt.xlabel('Average score')\n",
    "plt.ylabel('Number of SPIs')\n",
    "plt.show()\n",
    "\n",
    "# Load into a pandas series to pretty-print the results\n",
    "ser = pd.Series(scores)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(ser.dropna().sort_values())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We can inspect the MPIs to see what was the distinguishing feature for our top performing SPI\n",
    "\"\"\"\n",
    "\n",
    "# Get our top-performing SPI\n",
    "topspi = ser.index[ser.argmax()]\n",
    "print(f'Inspecting top SPI ({topspi}) in more detail...')\n",
    "\n",
    "# Unique time periods and datatypes\n",
    "periods = np.unique([k[1] for k in results])\n",
    "dtypes = ['stocks','forex']\n",
    "\n",
    "# Set up our axes\n",
    "fig, axs = plt.subplots(len(periods),2,figsize=(15,80))\n",
    "for (d, p) in results:\n",
    "    \n",
    "    # Get the MPI for this dataset\n",
    "    S = results[(d,p)][topspi]\n",
    "\n",
    "    # The axis position\n",
    "    i = [_i for _i, _p in enumerate(periods) if _p == p][0]\n",
    "    j = [_j for _j, _d in enumerate(dtypes) if _d == d][0]\n",
    "\n",
    "    # stocks or forex labels\n",
    "    labels = globals()[dtypes[j]]\n",
    "\n",
    "    # Plot the MPI\n",
    "    plot_mpi(S,topspi,labels,ax=axs[i,j])\n",
    "    plt.subplots_adjust(hspace=1.)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
